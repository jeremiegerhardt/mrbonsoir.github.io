<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>A color scientist on internet</title>
    <description>Jérémie Gerhardt or mrbonsoir is sharing his experience as a color scientist. He talks about the different projects he is conducting, taking part or simply interested in as well as sharing his notes about the conferences he is attending. The topics discussed are often located at the crossroad of color science, computer vision, VR, immersive cinema... What he tries to do is to popularize color science and to avoid taking too much about him at the third person because it's weird.
</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 08 May 2018 15:30:31 -0400</pubDate>
    <lastBuildDate>Tue, 08 May 2018 15:30:31 -0400</lastBuildDate>
    <generator>Jekyll v3.5.2</generator>
    
      <item>
        <title>Color blindness for the people</title>
        <description>&lt;h2 id=&quot;color-blindness&quot;&gt;Color blindness&lt;/h2&gt;

&lt;h2 id=&quot;color-blindness-simulation&quot;&gt;Color blindness simulation&lt;/h2&gt;

&lt;h2 id=&quot;lazyness&quot;&gt;Lazyness&lt;/h2&gt;

</description>
        <pubDate>Thu, 22 Feb 2018 09:22:06 -0500</pubDate>
        <link>http://localhost:4000/blog/post/2018/02/22/Color-blindness-for-the-people.html</link>
        <guid isPermaLink="true">http://localhost:4000/blog/post/2018/02/22/Color-blindness-for-the-people.html</guid>
        
        
        <category>blog</category>
        
        <category>post</category>
        
      </item>
    
      <item>
        <title>One Year in a startup</title>
        <description>&lt;h2 id=&quot;a-year-already&quot;&gt;A year already&lt;/h2&gt;

&lt;p&gt;One year in the same company, two countries and many more new stuffs learned during that time.&lt;/p&gt;

&lt;p&gt;Let’s re-introduce myself. I’m a color scientist and I have worked on various projects in the past 10 years, all of them having in common a signal to be processed, often the signal being an image visible by a human observer.&lt;/p&gt;

&lt;p&gt;I’m doing now r&amp;amp;d in a startup in Montréal, working on image quality and perceptual display. Being in a startup with experienced colleagues is a pretty good setup.&lt;/p&gt;

&lt;p&gt;On one side learning and improving an algorithm to improve image quality. The research point of view allows you to optimize, implement this algorithm. The goals are image quality and feasibility. You take your Matlab, your python, your pen and paper, process images, compute differences, eventually run psychophysic experiments, anything that can be done not in real time.&lt;/p&gt;

&lt;p&gt;On the other side, as I said I’m working in as startup (&lt;a href=&quot;http://www.irystec.com/&quot;&gt;IRYStec&lt;/a&gt;) and the challenge is to be able to quickly port a research project into production. Running your algorithm on your desktop computer is different than running it on a portable device in real time with more than one display technology… But that’s where it’s interesting too and that’s where you/I learn new stuffs.&lt;/p&gt;

&lt;h2 id=&quot;keep-learning&quot;&gt;Keep learning&lt;/h2&gt;

&lt;p&gt;Interesting because you have to learn new tools, here programming tools like openGL in order to program shader, re-think your algorithm for applying it in real time on a different hardware. And usually it’s less powerful than what you were used too… But it can help because it forces you to tackle your problem with another angle. You started with image processing, human vision system, perceptual display, JND and you finish with computer vision, electronic, hdr and shader programming.&lt;/p&gt;

&lt;p&gt;The evaluation of your work is different too. You don’t validate your solution with the same criteria if your are in research or in production. It seems obvious but your algorithm - if it has something to do with human vision - has quality cost (you want your image to be better or the original quality preserved such that perceptually the difference isn’t noticeable) and processing cost (if applying your new algorithm improves the visual quality but can only be ran at 10 fps it’s also a quality issue).&lt;/p&gt;

&lt;p&gt;Luckily you are not supposed to do everything, you and your colleagues have different skills, but you both should have crossover knowledge. Your supervisor should be able to fill the gap at the beginning or it’s like having two groups speaking a different language trying to solve the same problem hoping that magically they will understand each other. Understanding a minimum of how your office neighbor is working is more than relevant. It’s like your are on a fast car trying to catch a train and are looking for the jump that will help your to land on this moving train (this a beautiful picture of research and production).&lt;/p&gt;

&lt;h2 id=&quot;kafkaesque-bonus-track-oled-vs-lcd&quot;&gt;Kafkaesque bonus track, OLED vs LCD&lt;/h2&gt;

&lt;p&gt;OLED is trying to take over the world dominated by LCD technology. The big name are slowly switching to this technology. You can be almost sure that the middle names will follow until a new display technology arrives on the market. Take that planned obsolescence.&lt;/p&gt;

&lt;p&gt;What is confusing for consumers and researchers/engineers like me is the fact that two distinct TV displays can reach the same quality standard. Standard defined by experts, people from the industry. Standard saying these TVs have the same quality, also they produce different visual experience…&lt;/p&gt;

&lt;p&gt;It’s safe to say that if your setup is dark, no parasite lights around your TV, you may choose an OLED display to exploit the good &lt;em&gt;behavior&lt;/em&gt; of OLED in dark level, a pixel to 0 don’t produce light.&lt;/p&gt;

&lt;p&gt;It’s also safe to say that if your setup is quite bright, you will need a pretty bright display to compensate the ambient parasite light, therefore you may choose an LCD TV display. LCD are usually brighter than OLED, but it’s important too to distinguish between OLED TV display and portable OLED display…&lt;/p&gt;

&lt;p&gt;And we talk later about HDR which is pretty fun too. Especially when a device is said to be &lt;em&gt;HDR ready&lt;/em&gt; and is actually not an HDR display but a device that can perform &lt;em&gt;video tone mapping&lt;/em&gt; which is the action of mapping HDR content to a regular (or LDR) display. Here too there are plethora of HDR encoding :-)&lt;/p&gt;

</description>
        <pubDate>Tue, 06 Feb 2018 09:21:06 -0500</pubDate>
        <link>http://localhost:4000/blog/post/2018/02/06/One-year-in-a-startup.html</link>
        <guid isPermaLink="true">http://localhost:4000/blog/post/2018/02/06/One-year-in-a-startup.html</guid>
        
        
        <category>blog</category>
        
        <category>post</category>
        
      </item>
    
      <item>
        <title>CIC 25 in Lillehammer</title>
        <description>&lt;p&gt;This year have seen the 25th edition of the &lt;a href=&quot;https://www.imaging.org/site/IST/Conferences/Color_and_Imaging/IST/Conferences/CIC/CIC_Home.aspx&quot;&gt;Color Imaging Conference (CIC)&lt;/a&gt;) taking place in Lillehammer (Norway) after &lt;a href=&quot;http://mrbonsoir.github.io/blog/post/2016/11/12/CIC-in-San_Diego.html&quot;&gt;San Diego&lt;/a&gt; (California).&lt;/p&gt;

&lt;p&gt;This time again I could be part of the committee as a co-chairman for the &lt;strong&gt;Short Course&lt;/strong&gt; session. Session where conference attendees or not attendees can register and take courses (you need to pay for each course you want to follow) on carefully selected topics. Basis of color science courses are of course available but also several advance courses on different topics given by field specialists (e.g courses on hdr, color differences and other colorful themes). The instructors are usually coming from academia and/or industry - when their industry allows them to talk - and each course is first submitted, reviewed and/or not accepted.&lt;/p&gt;

&lt;h3 id=&quot;back-to-norway&quot;&gt;Back to Norway&lt;/h3&gt;

&lt;p&gt;I have been visiting Norway regularly after I left Gjovik in 2008 and its &lt;a href=&quot;https://www.ntnu.edu/colourlab&quot;&gt;colorlab&lt;/a&gt; (a few km South of Lillehammer) where I did my PhD on multispectral color reproduction. It was something to come back in the lab, seeing old colleagues, seeing that the color lab did grow nicely and was heavily represented at the conference.&lt;/p&gt;

&lt;h3 id=&quot;general-overview&quot;&gt;General overview&lt;/h3&gt;

&lt;p&gt;Over the years you appreciate the conference with different perspectives. Now that I know a bit more about my field - color science - I can witness that each sub-field is evolving slowly, appreciate tiny changes, observe the increase of general knowledge. There is always something.&lt;/p&gt;

&lt;h3 id=&quot;human-perception-and-colour-constancy&quot;&gt;Human perception and colour constancy&lt;/h3&gt;

&lt;p&gt;Keynotes, first and second have been pretty interesting. The first one given by Apple - almost did coincide with the Apple event in California - was impressive and disturbing in the same time. Impressive because of the amount of advanced technology they are able to squeeze into a smartphone. Disturbing by the non research dissemination they are doing. They have a great team of researchers and engineers but all the knowledge stay at work, not sure it will make the people smarter at the end. I’m sure they are not the only company doing that, but they are so big the result of such behavior has an impact.&lt;/p&gt;

&lt;p&gt;Second keynote was entitled &lt;em&gt;Twenty-five Years of Colour Constancy&lt;/em&gt; given by &lt;a href=&quot;http://www.ncl.ac.uk/ion/staff/profile/anyahurlbert.html#background&quot;&gt;Anya Hurlbert from Newcastle University (UK)&lt;/a&gt;. Needless to say it was pretty entertaining, a the typical presentation that make you feel smarter at the end. &lt;a href=&quot;https://en.wikipedia.org/wiki/Color_constancy&quot;&gt;Colour constancy&lt;/a&gt; is a fascinating phenomena which can be understood as white balancing done by the brain. We have not all the same automatic white balance algorithm embedded with us, this resulting
endless discussion on object color, remember the blue-black/yellow-white dress-gate a few month ago?&lt;/p&gt;

&lt;h3 id=&quot;subjective-evaluation-image-metrics-and-database&quot;&gt;Subjective evaluation, image metrics and database&lt;/h3&gt;

&lt;p&gt;There are so many metrics it’s becoming difficult to follow on that topic. To have a idea have a look at the &lt;a href=&quot;http://www.ponomarenko.info/tid2013.htm&quot;&gt;Tampere Image Database TID203&lt;/a&gt;. Each of them is trying to mimic how human perception is working, allowing to evaluate image difference, predict how image quality will be perceived when a new algorithm is applied to images.&lt;/p&gt;

&lt;p&gt;A solution is to follow an experimental workflow which integrate feedback from standard real observers. A panel of test persons is asked to look at images, original and modified and to compare them. Notion of &lt;a href=&quot;https://en.wikipedia.org/wiki/Just-noticeable_difference&quot;&gt;just noticeable difference (JND)&lt;/a&gt; is of course very important here. In that case subjective evaluation is often chosen, but here are well they is plenty of existing experimental workflows to choose from.&lt;/p&gt;

&lt;h3 id=&quot;human-vs-bird-viewing-system&quot;&gt;Human vs bird viewing system&lt;/h3&gt;

&lt;p&gt;Going to the same conference each year sometimes shows you how alone are  researchers. Most of them are spending their life on one topic and they become hyper specialist. A conference is the occasion for them to leave their laboratory, office and to share their discoveries (in that case other color scientists).&lt;/p&gt;

&lt;p&gt;A peculiar article tilted &lt;em&gt;Super Vision Model: What’s Peking Robin Seeing?&lt;/em&gt; by researcher Hiroaki Kotera from Kotera Imaging Laboratory, Chiba, Japan was pretty cool. This research work looked at birds (the Peking Robin) and their visual system: they are tetra-chromate &lt;em&gt;RGBU&lt;/em&gt; when we are tri-chromate &lt;em&gt;RGB&lt;/em&gt;. Question being how these animals see colors? &lt;em&gt;Four sensors and tiny brain Vs us three sensors and bigger brain that allows to post-process the signal and compensate for a lower captor resolution&lt;/em&gt; or how nature optimizes living organism for dedicated tasks within an ecosystem. The non-visible sensor allowing this bird to evaluate more precisely the quality of food (e.g. is this fruit already rotten or not?). A parallel can be done with computational photography when the algorithms and post-processing present in smartphone can compensate for their lower quality hardware (to some extend) compare to high-end dslr.&lt;/p&gt;

&lt;h3 id=&quot;workshop-session&quot;&gt;Workshop session&lt;/h3&gt;

&lt;p&gt;I could attend only two of the three parallel workshop sessions. interesting on many points. Both workshops &lt;em&gt;W2: VISUAL PERCEPTION AND EMERGING TECHNOLOGIES IN CINEMA: PERSPECTIVES FROM ACADEMIA AND THE INDUSTRY&lt;/em&gt; and &lt;em&gt;W3: CULTURAL HERITAGE DIGITIZATION: CHALLENGES AND OPPORTUNITIES&lt;/em&gt; shared cutting edge used of technologies. More important they allow to have discussions between professionals from different entities without revealing company secrets.&lt;/p&gt;

&lt;h3 id=&quot;la-suite&quot;&gt;La suite&lt;/h3&gt;

&lt;p&gt;Hope to be part of next year edition in 2018. For now the next destination hasn’t been revealed, but according to recent tradition it should return to North America.&lt;/p&gt;

</description>
        <pubDate>Fri, 15 Sep 2017 20:21:06 -0400</pubDate>
        <link>http://localhost:4000/blog/post/2017/09/15/CIC-Lillehammer.html</link>
        <guid isPermaLink="true">http://localhost:4000/blog/post/2017/09/15/CIC-Lillehammer.html</guid>
        
        
        <category>blog</category>
        
        <category>post</category>
        
      </item>
    
      <item>
        <title>Notes on NLP and color</title>
        <description>&lt;p&gt;This post is a never ending story and a follow up on a previous article on &lt;a href=&quot;http://mrbonsoir.github.io/blog/post/2014/05/28/Graph-en-stock.html&quot;&gt;graph and NoSql&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;how-to-organize-my-research&quot;&gt;How to organize my research?&lt;/h3&gt;
&lt;p&gt;I started to work on a few projects the last two months, projects involving color science, color space, finding the right JNDs (or Just Noticeable Differences) for our algorithms, display modeling and more.&lt;/p&gt;

&lt;p&gt;As usual with this type of R&amp;amp;D projects there is a bibliography stage in order to figure out what you understand about the topic and what have the people done about it. Pretty fast you are making cross references and identify which papers, authors, keywords are important to that topic. If you want to really understand and go further you need to read/re-do all of this work.&lt;/p&gt;

&lt;h3 id=&quot;déjà-view&quot;&gt;Déjà view&lt;/h3&gt;
&lt;p&gt;One more time when I’m facing this working situation where I’m missing a tool to represent the interactions between those publications. If you write a research article you pretty much have the connections between the works of others in your head, basically who is referring to whom, you tell your story while adding references in your writing instead of drawing a genealogy research tree. Having in my hand a tool to kind of visualize those connections will be nice as a support, something between illustration and text.&lt;/p&gt;

&lt;h3 id=&quot;the-database&quot;&gt;The database&lt;/h3&gt;
&lt;p&gt;A research article has always its sources listed with it, each source having as well links to other articles. The question I’m often facing is how do I build my DB - which I already have, e.g. &lt;strong&gt;bib&lt;/strong&gt; file containing a list of references with &lt;em&gt;year&lt;/em&gt;, &lt;em&gt;authors&lt;/em&gt;, &lt;em&gt;reference title&lt;/em&gt; and more - in order to request information I can’t directly see but only briefly grasp?&lt;/p&gt;

&lt;h3 id=&quot;the-requests-and-the-wishes&quot;&gt;The requests and the wishes&lt;/h3&gt;
&lt;p&gt;Regarding the requests I know what are the question I would like to have the answer from my DB:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;what is the degree of relationship between author A and author B.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In term of Natural Language Processing (NLP) I would be also really interesting to compare words used by different group of researchers. Ideally I could like to create:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;graphs of connections between researchers based on their publications&lt;/li&gt;
  &lt;li&gt;graphs of keywords to visualize how fields and sub-fields are represented.&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Mon, 10 Apr 2017 14:21:06 -0400</pubDate>
        <link>http://localhost:4000/blog/post/2017/04/10/Notes-on-NLP-and-color.html</link>
        <guid isPermaLink="true">http://localhost:4000/blog/post/2017/04/10/Notes-on-NLP-and-color.html</guid>
        
        
        <category>blog</category>
        
        <category>post</category>
        
      </item>
    
      <item>
        <title>CIC 2016 in San Diego</title>
        <description>&lt;p&gt;The &lt;a href=&quot;https://www.imaging.org/site/IST/Conferences/Color_and_Imaging/IST/Conferences/CIC/CIC_Home.aspx&quot;&gt;Color Imaging Conference&lt;/a&gt; just ended. This year it took place in San Diego CA and once again a good event! For the fourth time in a row I did join this event as a co-chair, this time for the &lt;strong&gt;short course&lt;/strong&gt; session that usually takes place the whole day before the conference program.&lt;/p&gt;

&lt;h3 id=&quot;what-did-we-learn-this-year&quot;&gt;What did we learn this year?&lt;/h3&gt;

&lt;p&gt;After attending many of this conference editions it’s easy to think that the field doesn’t move fast enough. But it’s not true, it’s going forward and it’s always a good exercise to de-construct or re-build the reasoning behind the presentations given by the different researchers and scientists.&lt;/p&gt;

&lt;h3 id=&quot;wide-gamut-vs-hdr-display&quot;&gt;Wide gamut vs hdr display&lt;/h3&gt;

&lt;p&gt;What I keep in my head this year are the discussions about display. I could join a particularly interesting course about &lt;strong&gt;color grading&lt;/strong&gt;, &lt;strong&gt;post-production&lt;/strong&gt; where attention was put on the following point: what are we talking about when we talk about &lt;strong&gt;wide gamut&lt;/strong&gt; and &lt;strong&gt;hdr display&lt;/strong&gt;? The question actually came during the course.&lt;/p&gt;

&lt;p&gt;The movie industry and especially the production and post-production parts offer a live laboratory to observe where the applied color science is going. It’s a battle where people try to bring the most realistic experience to the spectators in cinema theater or at home.&lt;/p&gt;

&lt;p&gt;The workflow developed for each movie is often on the verge of what is existing as often a unified commercial solution is not there yet, so you have to build it: one want to have more color and extend the gamut, one want to have an higher fps, another want to have a bigger dynamic of intensity, you see the challenges here.&lt;/p&gt;

&lt;h3 id=&quot;personalized-display&quot;&gt;Personalized display&lt;/h3&gt;

&lt;p&gt;Display is obviously a very hot topic of these days. The understanding we have now of how the human visual system works, human color perception, the possibility to evaluate one’s vision ability depending of your age, gender and many more factors is a golden mine.&lt;/p&gt;

&lt;p&gt;This means that &lt;strong&gt;each display content can be optimized&lt;/strong&gt; to each viewer. In some configuration it can be super important. If you take the car industry where displays are more and more present to help the driver, optimizing the displayed images such that the driver can get the most important information is essential for its safety and the people around.&lt;/p&gt;

</description>
        <pubDate>Fri, 11 Nov 2016 19:01:06 -0500</pubDate>
        <link>http://localhost:4000/blog/post/2016/11/11/CIC-in-San_Diego.html</link>
        <guid isPermaLink="true">http://localhost:4000/blog/post/2016/11/11/CIC-in-San_Diego.html</guid>
        
        
        <category>blog</category>
        
        <category>post</category>
        
      </item>
    
      <item>
        <title>Working with noise control</title>
        <description>&lt;p&gt;I like to write. I have to write often for work but I also enjoy to write for myself about &lt;a href=&quot;http://mrbonsoir.blogspot.com&quot;&gt;stuffs&lt;/a&gt; not specifically related to work. Writing anything requires concentration and sometimes it’s easy to search for distraction instead of clearing your head, focusing on the task to be done and doing it. Actually writing or reading both require a lot of concentration and over the years I develop a strategy when it’s time to think, write or read. Yes sometimes I think.&lt;/p&gt;

&lt;p&gt;The answer is music, or noise or even more precisely auto-generative music or self-generative noise. I don’t remember how I ended up on &lt;a href=&quot;http://www.noisli.com/&quot;&gt;noisli&lt;/a&gt; but since this discovery I intensively use this website/app to shape the sound atmosphere around me. You can mix together different background sounds, this is the idea, to create a noise mix if not musical that will keep you in your bubble, to surround you in a way that elements from the outside will not disturb you.&lt;/p&gt;

&lt;p&gt;Recreating the sound from the train or while being in an airplane works for me. I wish Miles Davis album &lt;a href=&quot;https://en.wikipedia.org/wiki/In_a_Silent_Way&quot;&gt;Silent Way&lt;/a&gt; could be played forever, with &lt;a href=&quot;http://www.noisli.com/&quot;&gt;noisli&lt;/a&gt; I can simulate the noise from a train ride. But the delicate sound from an FM frequency without voices is so relaxing to me.&lt;/p&gt;

&lt;p&gt;Now I’m not only using this app for working or to put myself into the quest for concentration, no. In winter I like to hear the sound of the fire crackling together with a light wind, it’s warming up my flat and my feet. The same in summer when it’s too warm, a bit of wind, a bit of lighting noise and/or the sound of light waves on the beach has a strong effect.&lt;/p&gt;

&lt;p&gt;Coming from the field of color science, computer vision, VR stuffs I find this kind of app extremely simple and super immersive. Video game designers didn’t wait for me to incorporate sound into their games to increase their immersive factor.&lt;/p&gt;

</description>
        <pubDate>Tue, 05 Apr 2016 09:01:06 -0400</pubDate>
        <link>http://localhost:4000/blog/post/2016/04/05/Working-with-noise.html</link>
        <guid isPermaLink="true">http://localhost:4000/blog/post/2016/04/05/Working-with-noise.html</guid>
        
        
        <category>blog</category>
        
        <category>post</category>
        
      </item>
    
      <item>
        <title>Uberisation</title>
        <description>&lt;p&gt;Many stories and paths were crossing this week in SF and around. After many trips to this area I had the chance, this time, to experience a bit more what will it be if I was living there. Personnaly I think that you get to know a place when you start to understand how people move in their living envirronment, when they commute to work, go shopping or having some social activities.&lt;/p&gt;

&lt;p&gt;I didn’t rent a car, again. A European reflex pushing me to use the local public transport. One thing you have to recognize is that the public transport isn’t very big in the US, except a few cities, but it’s pretty cheap and there isn’t so many real cities. So pretty soon you facing the dilema &lt;em&gt;I should have rented a car&lt;/em&gt;… Being in the Bay I thought why not trying &lt;a href=&quot;https://www.uber.com/&quot;&gt;uber&lt;/a&gt;? Asking people around uber or &lt;a href=&quot;https://www.lyft.com/&quot;&gt;lyft&lt;/a&gt; were coming first, the word taxi rarely mentioned and public transport always bringing an astonished face as reaction, like why will you do that, we are not in the third world country here or something like that. Therefore I tried, and needless to say it’s super convenient.&lt;/p&gt;

&lt;p&gt;But why is it working so well here and struggling a bit let says in Paris and Berlin? I also had the chance to meet employees from uber and we were discussing about these facts. One thing interesting here is the very low public transport offer and the reaction from the big companies in the Bay to create their own public transport… It’s very convenient for them as their employees can work more, right from the commute time. Also I heard that the bus and subway lines stop earlier than in NY, Paris or Berlin, so you will have to think of your transport before going out, it’s annoying, and the taxi drivers, I heard, because being in situtation of monopole could be picky when you requested a drive.&lt;/p&gt;

&lt;p&gt;All this uber-like companies are a treat for all work closed market or semi-old monopole companies. Especially when the targeted customers aren’t happy about the service adn dependant of this monopole. Ah lovely RER train drivers…&lt;/p&gt;

&lt;p&gt;I have the feeling the only improvement comes from offering a better service, nothing really new is brought, a kind a super fancy re-packaging. If you take Apple, they were rarely the first with their new products, but something different was added. Tesla is re-doing cars. When one company is attracting suddenly all the sun to itself, then the others start to be innovative again. Uber is not completely there in Berlin or Paris, since their offensive approach in Europe behaving in a typical American way, they manage to make the locals to react, at least it’s more convenient to order a taxi on your phone in Berlin right now. A bit of fear and the things move.&lt;/p&gt;

&lt;p&gt;And the last funny observation for is the abitlity to re-invet the wheel by tech companies. It’s possible to share your uber/lyft ride with other passangers, something between a bus and taxi. Something very similar to the aluguers in Capo Verde.&lt;/p&gt;

</description>
        <pubDate>Sat, 19 Mar 2016 17:25:06 -0400</pubDate>
        <link>http://localhost:4000/blog/post/2016/03/19/Ubersitation.html</link>
        <guid isPermaLink="true">http://localhost:4000/blog/post/2016/03/19/Ubersitation.html</guid>
        
        
        <category>blog</category>
        
        <category>post</category>
        
      </item>
    
      <item>
        <title>From IRL to VR</title>
        <description>&lt;p&gt;I’m working on a workshop proposal for the next &lt;a href=&quot;https://www.imaging.org/site/IST/Conferences/Color_and_Imaging/IST/Conferences/CIC/CIC_Home.aspx?hkey=d2cf3f19-87b4-4164-8274-c40180e9dfa7&quot;&gt;Color Imaging Conference&lt;/a&gt; November 7-11 in San Diego CA. The topic I want to discuss with the speakers and audience is VR. The idea is to present the state of the art of research - what’s happening in the labs - and how this technology is spread to the mass - from digital dome to VR headset.&lt;/p&gt;

&lt;p&gt;The acceptance of a new technology can be reach if normal users can embrace it. It’s a challenge for all scientists, engineers, interaction designers to give easy access to often very complicated stuffs. By complicated stuff here I mean the production of an immersive movie with one or several cameras. The 360 degrees VR photography scene is almost established (see the photographer work of &lt;a href=&quot;http://tanjabarnes.com/&quot;&gt;tanjabarnes&lt;/a&gt; and thanks to software tools such as &lt;a href=&quot;http://hugin.sourceforge.net/&quot;&gt;hugin&lt;/a&gt;) one person &lt;em&gt;alone&lt;/em&gt; can produce beautiful images. For immersive movies more resources are usually needed, digital dome and planetarium such as the &lt;a href=&quot;http://www.calacademy.org/incoming-trailer&quot;&gt;California Academy of Science&lt;/a&gt; have the team to create amazing contents for the visitors attending their shows.&lt;/p&gt;

&lt;p&gt;I do think there is still a gap for people to create CGI content by themself, even so &lt;a href=&quot;https://minecraft.net/&quot;&gt;minecraft&lt;/a&gt; or &lt;a href=&quot;https://www.buildwithchrome.com/builder&quot;&gt;lego with Chrome&lt;/a&gt; are contradicting me. But if you work already in a Virtual environment you eliminate many of the drawbacks usually present to generate multi-directional views: if you know where the objects in your scene are located, without talking about quality of rendering it’s &lt;em&gt;easy&lt;/em&gt; to generate views from virtual camera at various locations in space. So far I’m only describing techonological challenges and I’m not even tackling the story telling challenges in VR. To contradict me again, the people in the game industry didn’t wait for me to tell story within a virtual envirronment.&lt;/p&gt;

&lt;h3 id=&quot;a-few-words-about-my-background&quot;&gt;A few words about my background&lt;/h3&gt;
&lt;p&gt;I’m a &lt;a href=&quot;https://de.linkedin.com/in/jeremiegerhardt&quot;&gt;color scientist&lt;/a&gt;, but over the almost last 10 years I have been involved in various VR projects regarding image quality, stichting, camera and projector calibration. Projects being the technology behind immersive displays and more specifically how to control several displays for the projection on curved surfaces. An immersirve display can be a cave, cylinder to a full sphere in that matter.&lt;/p&gt;

&lt;p&gt;I had the chance to meet different communities, one of them being the &lt;a href=&quot;http://www.imersa.org/&quot;&gt;digital dome community&lt;/a&gt;. Interesting fact for me is that these people are usually understanding/developping their technology to be able to create immersive contents for the digital domes. Another aspect I find very interesting about VR, it is now somehow easier to &lt;a href=&quot;http://www.theverge.com/2016/3/14/11206552/vr-oculus-rift-htv-vive-valve-sony-psvr-GDC&quot;&gt;consume VR content&lt;/a&gt; &lt;em&gt;alone&lt;/em&gt; with your &lt;a href=&quot;https://www.oculus.com/&quot;&gt;smartphone&lt;/a&gt; when digital dome offer a &lt;em&gt;group&lt;/em&gt; immersive experience which is pretty cool too.&lt;/p&gt;

&lt;h3 id=&quot;description-of-the-workshop-draft&quot;&gt;Description of the workshop (draft)&lt;/h3&gt;

&lt;p&gt;The recent blossoming of VR headset has open the VR doors to new users. The technology is not anymore reserved to flight simulator, research center or high end gaming console. It is now relatively easy to consume VR content on the mobile display of your smartphone equipped with a &lt;a href=&quot;https://www.google.com/get/cardboard/&quot;&gt;google Cardboard&lt;/a&gt; in the most simple case.&lt;/p&gt;

&lt;p&gt;In that workshop we want to explore the different factors leading the user to have a successful VR experience. More specifically we will look at the solution existing for creating panorama to spherical movies using several cameras or using a single acquisition device, investigate the different scenario for live contents to recorded movies.&lt;/p&gt;

&lt;p&gt;The usual approach to create panorama movie is to combine several different fields of view, the operation of stitching will allow to blend each image into a single frame. This process have to deal with hdr/tone mapping, the choice of algorithm modify the final image quality which influence the perceived feeling of immersion.&lt;/p&gt;

&lt;p&gt;The general concept with immersive video is to consider the user in the middle of a sphere where the inner surface is the content of a frame. One aspect is to give the user the possibility to change its viewing direction by simply turning his head, a second aspect is to have the content ready at each frame. It’s possible to view immersive video on a regular video player where the viewing direction can be modified with a mouse (Youtube has this option), on the other side apps for VR headset will often use gaming tools (such as &lt;a href=&quot;https://unity3d.com/&quot;&gt;Unity Game engine&lt;/a&gt;), rendering tools (openGL…) to control the environment. From that point of view, app developers can work easily with texture, camera matrix projection, stitching functions…&lt;/p&gt;

&lt;h4 id=&quot;content-filmed-with-camera-cluster&quot;&gt;Content filmed with camera cluster&lt;/h4&gt;
&lt;p&gt;It is possible to combine several action cameras with a mounting rig, see &lt;a href=&quot;http://www.360heros.com/&quot;&gt;GoPro&lt;/a&gt;. As each camera position is fixed, only one configuration file is required to stitch automatically the images, see the &lt;a href=&quot;http://www.hhi.fraunhofer.de/departments/vision-imaging-technologies/products-technologies/capture/panoramic-uhd-video.html&quot;&gt;panoramic UHD video&lt;/a&gt; from &lt;a href=&quot;http://www.hhi.fraunhofer.de/start-page.html&quot;&gt;HHI&lt;/a&gt; in Berlin. Live streaming is possible, final quality is linked to the bandwidth, how fast can tone mapping be applied, how synchronized are the different camera streams (&lt;a href=&quot;http://www.video-stitch.com/&quot;&gt;Video-Stitch&lt;/a&gt;, &lt;a href=&quot;http://www.kolor.com/&quot;&gt;Kolor&lt;/a&gt; softwares can help). With this kind of installation ghosting effect can still be visible when an object is getting too close to the camera cluster and appear in only one camera field of view.&lt;/p&gt;

&lt;h4 id=&quot;content-filmed-with-a-single-camera-on-the-fly&quot;&gt;Content filmed with a single camera on the fly&lt;/h4&gt;
&lt;p&gt;It is possible if you can always consider your camera being located in the center of a sphere. Now giving the opportunity to the user to create himself with a single device an immersive movie is really interesting because it’s an invitation for him to take control on such new media/way of expression. And the challenges for the scientists and engineers to please the future immersive video directors are numerous as the users don’t really care about the science, it should work when you press the button (who knows about automatic white balance, autofocus, hdr, 3D, face detection, noise reduction, automatic quality improvement, photography in low light…). The challenge with a single camera is to able to follow the camera movement and to provide to the app this information under the form of a rotation matrix to process stitching in real time (&lt;a href=&quot;https://www.splashapp.co/&quot;&gt;viorama and splash&lt;/a&gt; can help).&lt;/p&gt;

&lt;h3 id=&quot;expected-outcome&quot;&gt;Expected outcome&lt;/h3&gt;

&lt;p&gt;The people attending the workshop should gain knowledge about the imaging workflow for VR application using their smartphone as display: what are the key points and challenges from applied research to production.&lt;/p&gt;

&lt;h3 id=&quot;how-to-participate&quot;&gt;How to participate?&lt;/h3&gt;

&lt;p&gt;Regarding the participation to the workshop as one of the speakers to share your experience regarding the challenges listed, 
please contact me jeremie[point]gerhardt[at]gmail[point]com .&lt;/p&gt;

&lt;p&gt;Check the conference webpage &lt;a href=&quot;https://www.imaging.org/site/IST/Conferences/Color_and_Imaging/IST/Conferences/CIC/CIC_Home.aspx?hkey=d2cf3f19-87b4-4164-8274-c40180e9dfa7&quot;&gt;Color Imaging Conference&lt;/a&gt; where there is much more to discover than the few lines above.&lt;/p&gt;

</description>
        <pubDate>Tue, 15 Mar 2016 05:25:06 -0400</pubDate>
        <link>http://localhost:4000/blog/post/2016/03/15/From-IRL-to-VR.html</link>
        <guid isPermaLink="true">http://localhost:4000/blog/post/2016/03/15/From-IRL-to-VR.html</guid>
        
        
        <category>blog</category>
        
        <category>post</category>
        
      </item>
    
      <item>
        <title>Ca continue aujourd'hui</title>
        <description>&lt;p&gt;With the help of my little fingers I finally open a second blog more decicated to my field of work. Color science in case… So far I only repost and correct some typo mistakes from my other blog &lt;a href=&quot;http://mrbonsoir.blogspot.com&quot;&gt;mrbonsoir à l’internet&lt;/a&gt; when I was talking about conference trips, workhsop. This to separate the &lt;em&gt;real&lt;/em&gt; me
and the &lt;em&gt;working&lt;/em&gt; me.&lt;/p&gt;

</description>
        <pubDate>Fri, 04 Mar 2016 10:25:06 -0500</pubDate>
        <link>http://localhost:4000/blog/post/2016/03/04/Ca-continue-aujourdhui.html</link>
        <guid isPermaLink="true">http://localhost:4000/blog/post/2016/03/04/Ca-continue-aujourdhui.html</guid>
        
        
        <category>blog</category>
        
        <category>post</category>
        
      </item>
    
      <item>
        <title>Algo in mobile phone camera</title>
        <description>&lt;p&gt;Smartphones are becoming our primary camera and our primary way to consume images. The technical differences between those devices are shrinking. How do you differentiate between them?&lt;/p&gt;

&lt;p&gt;Considering all devices coming with approximatively the same hardware from sensors to optic, the differences should come from the software. if you take Android OS, despite its presence on most of the smartphone, the manufacturers using this OS can still pre-load their devices with advanced options.&lt;/p&gt;

&lt;p&gt;My point here is not to list or say which one is better than the other, but to highlight all the options now available. What I think is interesting here is to see what is considered to be granted from the user point of view: auto-focus, white balancing, hdr, face detection, low light condition, panorama and to wonder what people really understood of these tools. The marketing department surely has something to say about the last point.&lt;/p&gt;

&lt;p&gt;To know a bit about the production workflow around this problematic, I really see the the battle of new algorithms/automatic image as the way for company to market their image (see the last &lt;a href=&quot;http://www.apple.com/iphone-6s/cameras/photos/&quot;&gt;iPhone campaign&lt;/a&gt;). A bit like the battle between film manufacturers (Kodak, Agfa, Fuji…) back in the days, where for the same ISO sensitivities (for me my favorite was 400 color by Fujifilm) the same color signal will appear slightly different on paper. The camera allows you to record the scene and the film/digital camera chosen will print the look of the final image.&lt;/p&gt;

&lt;p&gt;So what can you really do? A lot will answer the emergency color scientist. It is super important and very interesting to study how our visual system is functioning, how it adapts with the light condition (eg. how to improve an image when observe on your phone in low light condition). All sounds pretty cool to me, color scientists can be useful.&lt;/p&gt;

&lt;p&gt;La suite dans le futur.&lt;/p&gt;

</description>
        <pubDate>Wed, 02 Mar 2016 10:58:06 -0500</pubDate>
        <link>http://localhost:4000/blog/post/2016/03/02/algo-in-mobile-phone-camera.html</link>
        <guid isPermaLink="true">http://localhost:4000/blog/post/2016/03/02/algo-in-mobile-phone-camera.html</guid>
        
        
        <category>blog</category>
        
        <category>post</category>
        
      </item>
    
  </channel>
</rss>
