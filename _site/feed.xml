<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>A color scientist on internet</title>
    <description>Jérémie Gerhardt or mrbonsoir is sharing his experience as a color scientist. He  talks about the different projects he is conducting, taking part or simply interested in as well as sharing his notes about the conferences he is attending. The topics discussed  are often located at the crossroad of color science, computer vision, VR, immersive cinema...  What he tries to do is to popularize color science and to avoid taking too much about him  at the third person because it&#39;s weird.
</description>
    <link>http://mrbonsoir.github.io//</link>
    <atom:link href="http://mrbonsoir.github.io//feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Mon, 07 Mar 2016 16:01:27 +0100</pubDate>
    <lastBuildDate>Mon, 07 Mar 2016 16:01:27 +0100</lastBuildDate>
    <generator>Jekyll v3.0.3</generator>
    
      <item>
        <title>Ca continue aujourd&#39;hui</title>
        <description>&lt;p&gt;With the help of my little fingers I finally open a second blog more decicated to my field of work. Color science in case… So far I only repost and correct some typo mistakes from my other blog &lt;a href=&quot;http://mrbonsoir.blogspot.com&quot;&gt;mrbonsoir à l’internet&lt;/a&gt; when I was talking about conference trips, workhsop. This to separate the &lt;em&gt;real&lt;/em&gt; me
and the &lt;em&gt;working&lt;/em&gt; me.&lt;/p&gt;

</description>
        <pubDate>Fri, 04 Mar 2016 16:25:06 +0100</pubDate>
        <link>http://mrbonsoir.github.io//blog/post/2016/03/04/Ca-continue-aujourdhui.html</link>
        <guid isPermaLink="true">http://mrbonsoir.github.io//blog/post/2016/03/04/Ca-continue-aujourdhui.html</guid>
        
        
        <category>blog</category>
        
        <category>post</category>
        
      </item>
    
      <item>
        <title>Algo in mobile phone camera</title>
        <description>&lt;p&gt;Smartphones are becoming our primary camera and our primary way to consume images. The technical differences between those devices are shrinking. How do you differentiate between them?&lt;/p&gt;

&lt;p&gt;Considering all devices coming with approximatively the same hardware from sensors to optic, the differences should come from the software. if you take Android OS, despite its presence on most of the smartphone, the manufacturers using this OS can still pre-load their devices with advanced options.&lt;/p&gt;

&lt;p&gt;My point here is not to list or say which one is better than the other, but to highlight all the options now available. What I think is interesting here is to see what is considered to be granted from the user point of view: auto-focus, white balancing, hdr, face detection, low light condition, panorama and to wonder what people really understood of these tools. The marketing department surely has something to say about the last point.&lt;/p&gt;

&lt;p&gt;To know a bit about the production workflow around this problematic, I really see the the battle of new algorithms/automatic image as the way for company to market their image (see the last &lt;a href=&quot;http://www.apple.com/iphone-6s/cameras/photos/&quot;&gt;iPhone campaign&lt;/a&gt;). A bit like the battle between film manufacturers (Kodak, Agfa, Fuji…) back in the days, where for the same ISO sensitivities (for me my favorite was 400 color by Fujifilm) the same color signal will appear slightly different on paper. The camera allows you to record the scene and the film/digital camera chosen will print the look of the final image.&lt;/p&gt;

&lt;p&gt;So what can you really do? A lot will answer the emergency color scientist. It is super important and very interesting to study how our visual system is functioning, how it adapts with the light condition (eg. how to improve an image when observe on your phone in low light condition). All sounds pretty cool to me, color scientists can be useful.&lt;/p&gt;

&lt;p&gt;La suite dans le futur.&lt;/p&gt;

</description>
        <pubDate>Wed, 02 Mar 2016 16:58:06 +0100</pubDate>
        <link>http://mrbonsoir.github.io//blog/post/2016/03/02/algo-in-mobile-phone-camera.html</link>
        <guid isPermaLink="true">http://mrbonsoir.github.io//blog/post/2016/03/02/algo-in-mobile-phone-camera.html</guid>
        
        
        <category>blog</category>
        
        <category>post</category>
        
      </item>
    
      <item>
        <title>A glimpse of code</title>
        <description>&lt;p&gt;Having a lot of time pushes me - I hope - to be organised. So in an attempt to sort my data - mostly pieces of code on Python - in order to make it available when I need it I made a new github repository where I store iPython notebook with a few explanations.&lt;/p&gt;

&lt;p&gt;Everything is &lt;a href=&quot;https://github.com/mrbonsoir/random_notebooks&quot;&gt;here&lt;/a&gt; and maybe it will be of some people interests, or not.&lt;/p&gt;

</description>
        <pubDate>Wed, 17 Feb 2016 17:15:06 +0100</pubDate>
        <link>http://mrbonsoir.github.io//blog/post/2016/02/17/a-glimpse-of-code.html</link>
        <guid isPermaLink="true">http://mrbonsoir.github.io//blog/post/2016/02/17/a-glimpse-of-code.html</guid>
        
        
        <category>blog</category>
        
        <category>post</category>
        
      </item>
    
      <item>
        <title>CIC visiting Darmstadt</title>
        <description>&lt;p&gt;What is CIC you may ask yourself? It’s stand for &lt;a href=&quot;http://www.imaging.org/site/IST/Conferences/Color_and_Imaging/IST/Conferences/CIC/CIC_Home.aspx?hkey=d2cf3f19-87b4-4164-8274-c40180e9dfa7&quot;&gt;Color Imaging Conference&lt;/a&gt;, a conference about color and imaging. This year it took place in Darmstadt DE. The last 22 editions always took place in the US, last year it was in Boston MA, two years ago in Albuquerque NM, three years ago in Los Angeles CA, four years ago in San Antonio TX and that’s it for my involvement. Next stop is San Diego CA in November 2016.&lt;/p&gt;

&lt;p&gt;I’m a regular attendee, I joined this community already ten years ago alternating between CGIV, &lt;a href=&quot;http://www.aic-colour.org/&quot;&gt;AIC&lt;/a&gt;, &lt;a href=&quot;http://www.imaging.org/ist/Conferences/ei2016/&quot;&gt;EI&lt;/a&gt; and CIC. Depending of the event you will meet a slightly different crowd or so to say different crowds will meet allowing to go deeper in the various fields represented. But for sure it’s about imaging, color, perception, printing, archiving, image acquisition, color management, camera and display calibration, gamut mapping and more.&lt;/p&gt;

&lt;p&gt;This year almost 200 persons were attending the event in Darmstadt. There is a kind of routine in such event and being part of the committee allows you to see the people interaction with a special look. It’s very special to see the attendees - former colleagues, friends, known members of this community - arriving from everywhere almost - from North America, Europe, Asia, Australia… - and being all jet-lagged. Even if you are traveling in the same time zone you will end up jet-lagged. First of all the schedule is tide and you have to use the “free” time to talk with everybody. Sharing a meal or a beer is usually very appropriate. As a result you barely have time to rest, but the kind of adrenaline you get from meeting the crème de la crème of the color scientists keeps you awake.&lt;/p&gt;

</description>
        <pubDate>Fri, 23 Oct 2015 18:23:06 +0200</pubDate>
        <link>http://mrbonsoir.github.io//blog/post/2015/10/23/CIC-visiting-Darmstadt.html</link>
        <guid isPermaLink="true">http://mrbonsoir.github.io//blog/post/2015/10/23/CIC-visiting-Darmstadt.html</guid>
        
        
        <category>blog</category>
        
        <category>post</category>
        
      </item>
    
      <item>
        <title>About not being an expert as a data scientist and other tech stuffs</title>
        <description>&lt;p&gt;Last evening I did attend a joined Meetup from the Python User Berlin (PUB) and the Zalando Tech Event hosted by Zalando and offering talks on Natural Langage Processing (NLP). Both talks went well and gave two views on the topic: one on the state of the art of the tools for NLP using Python and a second more applied.&lt;/p&gt;

&lt;p&gt;The discussions I add after while enjoying a club mate - la boisson des champions - were equally interesting. First of all I started discussing with a expert of NLP trying to explain why I joined this event and what was my link with NLP. In my very recent job experience at EyeEm I just touched the surface of NLP preparing data for Machine Learning (ML) using nltk together with WordNet, ImageNet. Actually I didn’t do much of text analysis but batching word definition. In that experiment the text analysis will have come after this step and that’s where semantic is jumping into the discussion. Because working with the word dictionary is one side of the problem: you have one word with its definition and often - at least with scientists or engineers - you are in the inverse configuration which is you having words when actually you want to extract a definition, an idea, an information… And I let you google automatic image tagging, deep learning.&lt;/p&gt;

&lt;p&gt;After exchanging ideas and experiences about NLP I did continue seeping the offered mate with one Zalando employee. I was curious - as usual - to understand what it means to be a data scientist here. Because if the definition is very general - a data scientist works with data, we are not experts - it’s interesting to see how many fields we - I’m one of those people - cover in our daily work. Using the same language - e.g. Python - we can go from signal processing, computer vision, image retrieval, NLP, how to deal with Databases - a year ago I wrote on the topic Databases and natural Langage graphs en stock - how to present your results to non expert by doing nice visualization and many more… So if we are not expert we need to be pretty fast I acquiring skills from various fields and/or use the appropriate tools.&lt;/p&gt;
</description>
        <pubDate>Wed, 30 Sep 2015 12:50:06 +0200</pubDate>
        <link>http://mrbonsoir.github.io//blog/post/2015/09/30/About-not-being-an-expert-as-a-data-scientist.html</link>
        <guid isPermaLink="true">http://mrbonsoir.github.io//blog/post/2015/09/30/About-not-being-an-expert-as-a-data-scientist.html</guid>
        
        
        <category>blog</category>
        
        <category>post</category>
        
      </item>
    
      <item>
        <title>Clash of the titans - optimization vs. machine learning</title>
        <description>&lt;h3 id=&quot;in-the-beginning&quot;&gt;In the beginning&lt;/h3&gt;
&lt;p&gt;What is important to know about machine/deep learning problems? First remark to myself is “what are we trying to solve in general?” and then “which method/technique do we choose?” or “which approach is most appropriate to answer a given problem?”.&lt;/p&gt;

&lt;h3 id=&quot;optimization-for-the-people&quot;&gt;Optimization for the people&lt;/h3&gt;
&lt;p&gt;Optimization is widely used to solve complex problems that don’t have an analytic expression. But this doesn’t mean that problems that have an analytic expression couldn’t be solved using optimization.&lt;/p&gt;

&lt;p&gt;Optimization relies on a provided model that “model” with reasonable efficiency a phenomenon (e.g. find the colorant combination of cyan, magenta, yellow and more for a give red, green, blue pixel) or anything you want. You may hear about derivatives, gradient, local minimum, cost function, quadratic form, linearity, non-linearity, iteration and more when you start messing around with optimization.&lt;/p&gt;

&lt;p&gt;And it’s completely possible to use optimization techniques as applied mathematics tools without knowing exactly how they work (e.g. you provide your model and the tools will perform the derivatives for you). In an engineering world you are connecting boxes, each one trying to solve a simple task taking for starting point what the previous is having for output.&lt;/p&gt;

&lt;h3 id=&quot;deep-learning-for-the-people&quot;&gt;Deep learning for the people&lt;/h3&gt;
&lt;p&gt;Deep learning and neural networks let you do something clever with the way to solve your problem. First of all your problem has been defined and described, but optimization techniques did not provide expected results: it’s not fast enough or it’s simply not working. One possibility is that your model isn’t good enough or way to complex.&lt;/p&gt;

&lt;p&gt;The simple idea is to let a system to learn about an ecosystem. To do so we let the algorithms mimicking how our brain is working. The concept of learning is very important here because it is really what we want to achieve. We want that our algorithm learns in a first step by obtaining representative parameters/weights before giving us a result. Then once the learning is finished, for a given entree and with the help of the parameters the algorithm can give us answers. For example is this image an image of a car, an elephant and this with different degrees of confidence.&lt;/p&gt;

&lt;p&gt;A big part of the learning is to prepare the training sample. You can’t just give images to the algorithm. Applied to computer vision, deep learning methods try to extract features from images in a similar way of how we human recognize information in images. This step of features extraction goes by applying multiple filtering on the images and the resulting filtered images, using convolution and tile approaches. At the end you obtain classes of features and it’s very similar to the filters used for face recognition. Only difference is the features that describe a human face are now almost standard and doesn’t need to be computed or extracted again.&lt;/p&gt;

&lt;p&gt;There exist competitions where for a given large database full of images and  keywords, people can submit their algorithms. Pretty interesting results are obtained and as in sport faster solutions are appearing often coming with new tools to handle large databases.&lt;/p&gt;

&lt;h3 id=&quot;breaking-the-machine&quot;&gt;Breaking the machine&lt;/h3&gt;
&lt;p&gt;Hopefully there is always something to improve. Because images can contain more than one object, you could have a bike and an elephant in the same picture. In that case what should reply our algorithm first? There is room for subjectivity here.&lt;/p&gt;

&lt;p&gt;These algorithms have to deal with the constant stream of information we are processing, meaning that we are always learning - in theory of course because the world is full of lazy bastards which keeps the marketing and sales people happy making us predictable and therefore easy targets but I digress - and we have to find a way to give this ability to our algorithms or there is the risk for them to over-learn. I really like the metaphor of trying to make an algorithm able to forget part of what he is deep learning to be able to adjust its judgement.&lt;/p&gt;

&lt;p&gt;The interesting problems are those that overcome the first limitations encountered. You could try to distinguish what are the elements in a picture (e.g. there is an elephant and a bike) or “simply” give to the image a score. If you take an artist, he will have the tendency not only to make the same picture but to add to its images something that defines the way he perceive the world around him, something pretty unique. A similarity factor or score can be very helpful when you are browsing a large image databases or “just” the internet.&lt;/p&gt;
</description>
        <pubDate>Wed, 15 Apr 2015 13:17:06 +0200</pubDate>
        <link>http://mrbonsoir.github.io//blog/post/2015/04/15/Clash-of-the-titans.html</link>
        <guid isPermaLink="true">http://mrbonsoir.github.io//blog/post/2015/04/15/Clash-of-the-titans.html</guid>
        
        
        <category>blog</category>
        
        <category>post</category>
        
      </item>
    
      <item>
        <title>Deep learning (ou deep learning in French)</title>
        <description>&lt;h3 id=&quot;what-was-your-question-already&quot;&gt;What was your question already?&lt;/h3&gt;
&lt;p&gt;How to explain deep learning to your friends, family members, neighbors, random stranger, dog? A very good question indeed. Rather than going deeply into neural networks and other festivities let’s start with describing the problem we want to solve or at least let’s give an example of what we are trying to do here.&lt;/p&gt;

&lt;p&gt;Over the years I had to come with strategies if I wanted to explain what I do for living. Telling combinations of words such as “color science”, “computer vision”, “image processing”, “digital photography” is usually not enough or saying “I do work with images” neither. I always found interesting to answer the question “why to you want to do that?” or “which problem do you want to solve?” instead of saying I’m this or that. So to explain what I can do I try to give an idea of the tasks I have to solve.&lt;/p&gt;

&lt;h3 id=&quot;what-is-the-problem-you-are-trying-to-solve-already&quot;&gt;What is the problem you are trying to solve already?&lt;/h3&gt;
&lt;p&gt;In some way asking these questions is already machine learning/deep learning-ish approach of solving a problem. In theory if someone asks you to solve a problem he knows the kind of results he want to obtain for a given input or starting point. What he doesn’t know is what is happening between these two stages. Applied mathematics and optimization are a reasonable standard solution: you develop of model that recreate more-less accurately what is happening between these two stages, then for a new entree point your model will predict what an output will be.&lt;/p&gt;

&lt;p&gt;I’m sure &lt;em&gt;big data&lt;/em&gt; is an expression you have heard in the past years or months. It has of course different meaning depending who is giving a definition. But, coming back to images and the incredible amount of images we are producing daily there is a need to develop solutions/tools to be able to interact with these images. You have in your hand an extremely large image database and using keyword as a search query isn’t enough anymore. Here is the problem: how to navigate, how to browse into large image database in a more natural way? There is a bit of database here but that is not the main point of my article, check my past post on graph and database if you are interested.&lt;/p&gt;

&lt;h3 id=&quot;face-recognition-to-recognition-of-everything&quot;&gt;Face recognition to recognition of everything&lt;/h3&gt;
&lt;p&gt;Working with images is fascinating, you see one image and automatically you extract some of its  information. Of course there is a long learning curve, when you see a tree, a car, a known object in a picture you don’t even realize it, you know, you have learned over the years you spent on earth to recognize, categorize, organize the continuous stream of visual information that come to your eyes and is later processed in your brain.&lt;/p&gt;

&lt;p&gt;If you think of face recognition, the mathematical tools are now pretty standard. We can with high probability find out faces in images, classification comes after the recognition. And if you train your model you will be able to recognize semi automatically in a database faces of different persons as the tools/filters can be tuned for a given target. It can be scary of course if the threshold that decide for a true recognition/classification isn’t verified by a real human and that action leads to a rocket launch. Actually any automatic action issued from an algorithm decision having impact on a human being is pretty bad (hello mass surveillance and hello Terminator). You want help from robots not to help robots or it’s too late anyway.&lt;/p&gt;

&lt;iframe src=&quot;https://player.vimeo.com/video/12774628&quot; width=&quot;500&quot; height=&quot;477&quot; frameborder=&quot;0&quot; webkitallowfullscreen=&quot;&quot; mozallowfullscreen=&quot;&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;p&gt;&lt;a href=&quot;https://vimeo.com/12774628&quot;&gt;OpenCV Face Detection: Visualized&lt;/a&gt; from &lt;a href=&quot;https://vimeo.com/adamhrv&quot;&gt;Adam Harvey&lt;/a&gt; on &lt;a href=&quot;https://vimeo.com&quot;&gt;Vimeo&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;An idea behind deep learning is to be able to learn what are into images - in a similar way as we human do - to extract features and to perform tasks on other images based on a trained neural network. I’m making shortcuts but that’s the idea. To understand and to later mimic how information is circulating into the brain has been a dream of many researchers. Neural networks go into that direction. If a few years ago the algorithms were limited because of computer power the global picture is different now.&lt;/p&gt;

&lt;p&gt;What is also interesting is that new strategies had to be developed to overcome the overload of data. In a way the systems were “over learning” and people talked about over-fitting the data. And it makes sens. If I’m not too mistaken our brain is not indefinitely expandable, meaning we are sorting information continuously. One big part of these tools is to perform drop-out which can be explained as “now that our system can learn we have to teach him to forget part of what he knows in real time”.&lt;/p&gt;

&lt;h3 id=&quot;cross-disciplines&quot;&gt;Cross disciplines&lt;/h3&gt;
&lt;p&gt;A chance I see - for me - is the need in some industries for expert being not only expert in one field. Specially for this kind of large scale problems involving images, computer vision, real time and fancy applied research projects. To know only about machine learning or statistic is not enough, to know both about computer and machine learning tools is better.&lt;/p&gt;

</description>
        <pubDate>Thu, 09 Apr 2015 13:28:06 +0200</pubDate>
        <link>http://mrbonsoir.github.io//blog/post/2015/04/09/Deep-learning.html</link>
        <guid isPermaLink="true">http://mrbonsoir.github.io//blog/post/2015/04/09/Deep-learning.html</guid>
        
        
        <category>blog</category>
        
        <category>post</category>
        
      </item>
    
      <item>
        <title>CIC 22 in Boston</title>
        <description>&lt;p&gt;For its 22th edition the Color Imaging Conference or CIC for the connsoisseurs took place in Boston. Comparing to last year in Albuquerque (NM) the color LUT did change a bit: yellow and blue for the desert and the sky to red and about 50 shades of autumn for the buildings and trees.&lt;/p&gt;

&lt;p&gt;If as the previous editions the crowd of attendees remains similar - la crème de la crème of the color scientists - you can detect a slight dominance according to the conference location or with whom is organized the event. This year the keynote sessions where for both color scientists (CIC) and medical imaging scientists (IADP), then sessions were ran in parallel.&lt;/p&gt;

&lt;p&gt;The joint keynotes brought interesting discussions. They were highlighting another practices of imaging system - like microscope - in the medical world. That is not a surprise that the imaging system described can provide very different visual results, from one microscope to the other the data do not appear identical to human observers but the extraction of information should. In one word there is no reference images or colors when the practicians are looking at samples, but they know that each patient is different and can base their diagnostic on their experience. In comparison to the CARS event, the IADP was much less about computer assisted radiography and more about image analysis (at least for what I have seen).&lt;/p&gt;

&lt;p&gt;The day before the real opening of the conference is the short course day. Professionals, experts in academia or industry are giving lectures on color science topics. After chairing this session last year in Albuquerque NM I could follow two courses in Boston: one on color display given by Gabriel Marcu from Apple and a second on Color Rendering Index and Lighting given by Wendy Davis from University of Sydney.  It is always refreshing to take part into these courses, to re-learn about display technology, display calibration, lighting technology and equally important how our comprehension of color science has evolves over the years and how new technologies force to change our approaches.&lt;/p&gt;

&lt;p&gt;One thing I remind in particular is the new range of LEDs available - for some years already - and the changes that go with them. It is possible to design the properties of your lights - its spectral power distribution SPD - at very narrower wavelength bands. What does it mean? It means that you can really shape the gamut of your display or light installation, this in term of triangle shape in the color space of your choice. But being able to define a larger, wider color gamut on a sketch is not or should not be the only goal. We also have to think of the internal color space distribution, after all the light source we are most of the time in contact - the sun - has a very different SPD. The good side of it is the door open for us color scientists to continue exploring these new spaces.&lt;/p&gt;

&lt;p&gt;It was good to see how my field of studies during my PhD - multi-spectral color reproduction, multi-colorant printing - has evolved. How multi-ink printing systems that were semi-experimental some years ago are now the basis of further experimentation. Further more it always a pleasure to hear someone telling you “I have read your thesis” when they see your name and your face for the first time.&lt;/p&gt;
</description>
        <pubDate>Fri, 07 Nov 2014 22:26:06 +0100</pubDate>
        <link>http://mrbonsoir.github.io//blog/post/2014/11/07/CIC-in-Boston.html</link>
        <guid isPermaLink="true">http://mrbonsoir.github.io//blog/post/2014/11/07/CIC-in-Boston.html</guid>
        
        
        <category>blog</category>
        
        <category>post</category>
        
      </item>
    
      <item>
        <title>Boston 13 years later</title>
        <description>&lt;p&gt;Second time in Boston, about 13 years after my first and only time in 2001 when I was an exchange student in Montréal. For the first trip a car full of Frenchies and Belgies was going South, leaving Québec behind us for the a long weekend. As far as I remember it was a really good weekend, not legendary but still very ok.&lt;/p&gt;

&lt;p&gt;But back to our actual trip. I’m arriving from Berlin via Amsterdam. Luckily the Delta Airlines plane is pretty empty - I have two seats for me alone - and equipped with fairly new personal screen, I can actually see something - ce qui n’est pas toujours du luxe. I can say I was efficient in term of watching videos.&lt;/p&gt;

&lt;p&gt;Landing was smooth, so was the custom and retrieving my luggage. In the plane I met a fellow going to attend the same conference as me. The reason I’m travelling to Boston is to attend the Color Imaging Conference (CIC22 to be precised). For once we left the South states of New Mexico, Texas or California, no excursion to the desert this time.&lt;/p&gt;

&lt;p&gt;But back to Boston as I’m diverging again. The city isn’t too big and the public transport is actually existing. I jumped in the subway and reached shortly after landing my AirBnB apartment in South End, somehow the South. Neat. Especially neat if you have seen the average price for an hotel room this week.&lt;/p&gt;

&lt;p&gt;Thanks to the conference organisation I’m able to attend the event another time. In a smaller city all attendees are usually staying in the same hotel. This time not. Meaning I will have to wait tomorrow to meet the color scientist fellows, except one coming from France with whom I’m sharing the flat. According to plan and his plane he should arrive soon.&lt;/p&gt;
</description>
        <pubDate>Mon, 03 Nov 2014 19:28:06 +0100</pubDate>
        <link>http://mrbonsoir.github.io//blog/post/2014/11/03/Boston-13-years-later.html</link>
        <guid isPermaLink="true">http://mrbonsoir.github.io//blog/post/2014/11/03/Boston-13-years-later.html</guid>
        
        
        <category>blog</category>
        
        <category>post</category>
        
      </item>
    
      <item>
        <title>Color science for beginners</title>
        <description>&lt;p&gt;I’m a color and imaging scientist who does photography, but sometimes I’m a photographer who does color and imaging research. Depending of the moment and the project one or the other will be predominant.&lt;/p&gt;

&lt;p&gt;The practise of photography is always interesting to remind how the light is captured, how images are made. Know the acquisition process in details - light condition, lens used, subject or content - will always help when it is time to work on the images - be it for displaying the resulting images or for extracting automatically information from them.&lt;/p&gt;

&lt;p&gt;These few lines are only a glimpse of what color science is. In my many attempts to explain what a color scientist does I came up with this little definition: a color scientist deals with light, its acquisition, preservation and reproduction, of course he also works with images. The term “color” refer to the visible spectrum - a color scientist is a multispectral imaging scientist with limited spectral boundaries - and by adding the word “visible” - visible to the human eye of course - we just extended the range of possible activities such as studying how a human eye does function, how do we perceive light signals, read images. From physics we come to philosophy.&lt;/p&gt;

&lt;p&gt;Engineering projects which involve to work with images are often straightforward: you have an image, you need to detect some information, you use a define metric and it is done - almost of course. A color imaging project which involves art, artists and their work is different. Artists and scientists do not speak the same language, they may use the same tools but with different guidelines for sure… But that’s where the fun comes in.&lt;/p&gt;
</description>
        <pubDate>Tue, 07 Oct 2014 12:00:06 +0200</pubDate>
        <link>http://mrbonsoir.github.io//blog/post/2014/10/07/Color-science-for-beginners.html</link>
        <guid isPermaLink="true">http://mrbonsoir.github.io//blog/post/2014/10/07/Color-science-for-beginners.html</guid>
        
        
        <category>blog</category>
        
        <category>post</category>
        
      </item>
    
  </channel>
</rss>
