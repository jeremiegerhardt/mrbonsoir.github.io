<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>A color scientist on internet</title>
    <description>Jérémie Gerhardt or mrbonsoir is sharing his experience as a color scientist. He  talks about the different projects he is conducting, taking part or simply interested in as well as sharing his notes about the conferences he is attending. The topics discussed  are often located at the crossroad of color science, computer vision, VR, immersive cinema...  What he tries to do is to popularize color science and to avoid taking too much about him  at the third person because it&#39;s weird.
</description>
    <link>http://mrbonsoir.github.io//</link>
    <atom:link href="http://mrbonsoir.github.io//feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Wed, 09 Mar 2016 11:15:42 +0100</pubDate>
    <lastBuildDate>Wed, 09 Mar 2016 11:15:42 +0100</lastBuildDate>
    <generator>Jekyll v3.0.3</generator>
    
      <item>
        <title>Ca continue aujourd&#39;hui</title>
        <description>&lt;p&gt;With the help of my little fingers I finally open a second blog more decicated to my field of work. Color science in case… So far I only repost and correct some typo mistakes from my other blog &lt;a href=&quot;http://mrbonsoir.blogspot.com&quot;&gt;mrbonsoir à l’internet&lt;/a&gt; when I was talking about conference trips, workhsop. This to separate the &lt;em&gt;real&lt;/em&gt; me
and the &lt;em&gt;working&lt;/em&gt; me.&lt;/p&gt;

</description>
        <pubDate>Fri, 04 Mar 2016 16:25:06 +0100</pubDate>
        <link>http://mrbonsoir.github.io//blog/post/2016/03/04/Ca-continue-aujourdhui.html</link>
        <guid isPermaLink="true">http://mrbonsoir.github.io//blog/post/2016/03/04/Ca-continue-aujourdhui.html</guid>
        
        
        <category>blog</category>
        
        <category>post</category>
        
      </item>
    
      <item>
        <title>Algo in mobile phone camera</title>
        <description>&lt;p&gt;Smartphones are becoming our primary camera and our primary way to consume images. The technical differences between those devices are shrinking. How do you differentiate between them?&lt;/p&gt;

&lt;p&gt;Considering all devices coming with approximatively the same hardware from sensors to optic, the differences should come from the software. if you take Android OS, despite its presence on most of the smartphone, the manufacturers using this OS can still pre-load their devices with advanced options.&lt;/p&gt;

&lt;p&gt;My point here is not to list or say which one is better than the other, but to highlight all the options now available. What I think is interesting here is to see what is considered to be granted from the user point of view: auto-focus, white balancing, hdr, face detection, low light condition, panorama and to wonder what people really understood of these tools. The marketing department surely has something to say about the last point.&lt;/p&gt;

&lt;p&gt;To know a bit about the production workflow around this problematic, I really see the the battle of new algorithms/automatic image as the way for company to market their image (see the last &lt;a href=&quot;http://www.apple.com/iphone-6s/cameras/photos/&quot;&gt;iPhone campaign&lt;/a&gt;). A bit like the battle between film manufacturers (Kodak, Agfa, Fuji…) back in the days, where for the same ISO sensitivities (for me my favorite was 400 color by Fujifilm) the same color signal will appear slightly different on paper. The camera allows you to record the scene and the film/digital camera chosen will print the look of the final image.&lt;/p&gt;

&lt;p&gt;So what can you really do? A lot will answer the emergency color scientist. It is super important and very interesting to study how our visual system is functioning, how it adapts with the light condition (eg. how to improve an image when observe on your phone in low light condition). All sounds pretty cool to me, color scientists can be useful.&lt;/p&gt;

&lt;p&gt;La suite dans le futur.&lt;/p&gt;

</description>
        <pubDate>Wed, 02 Mar 2016 16:58:06 +0100</pubDate>
        <link>http://mrbonsoir.github.io//blog/post/2016/03/02/algo-in-mobile-phone-camera.html</link>
        <guid isPermaLink="true">http://mrbonsoir.github.io//blog/post/2016/03/02/algo-in-mobile-phone-camera.html</guid>
        
        
        <category>blog</category>
        
        <category>post</category>
        
      </item>
    
      <item>
        <title>A glimpse of code</title>
        <description>&lt;p&gt;Having a lot of time pushes me - I hope - to be organised. So in an attempt to sort my data - mostly pieces of code on Python - in order to make it available when I need it I made a new github repository where I store iPython notebook with a few explanations.&lt;/p&gt;

&lt;p&gt;Everything is &lt;a href=&quot;https://github.com/mrbonsoir/random_notebooks&quot;&gt;here&lt;/a&gt; and maybe it will be of some people interests, or not.&lt;/p&gt;

</description>
        <pubDate>Wed, 17 Feb 2016 17:15:06 +0100</pubDate>
        <link>http://mrbonsoir.github.io//blog/post/2016/02/17/a-glimpse-of-code.html</link>
        <guid isPermaLink="true">http://mrbonsoir.github.io//blog/post/2016/02/17/a-glimpse-of-code.html</guid>
        
        
        <category>blog</category>
        
        <category>post</category>
        
      </item>
    
      <item>
        <title>Some data about a data conference</title>
        <description>&lt;p&gt;I had the chance to attend the [Data Natives][link-datanatives] 2015 event last week in Berlin. A first time event having for topics FinTech, IoT and of course Big Data. I heard some interesting talks but also less interesting ones. You can still hear people having the dream of forecasting anything with the help of more data, but with often the feeling it’s only in order to sale more stuffs. I haven’t got the life improvement that suppose to go with Big Data (e.g. mass surveillance doesn’t work obviously).&lt;/p&gt;

&lt;p&gt;But here and there you can sometimes hear someone talking about a project that hold your attention. For me the most interesting aspect is the inter-disciplinary or multi-disciplinary aspect of the data. To achieve something relevant or meaningful with all the available information, you need to be able to define first what is the problem you are trying to solve (and yes I have a degree in opening open door).&lt;/p&gt;

&lt;p&gt;Four presentations are still in my head, one using NLP to pre-sort a lot of CV (from HitFox during the first day of the program) and a second using computer vision to automatically give feedback on webpage design (from EyeQuant last talk of the second day). Actually for the last one their talk was much wider than this single problem.&lt;/p&gt;

&lt;p&gt;About IoT and FinTech is wasn’t really impressive. Actually the only striking aspect is that the same tools are used whatever is your field of work (like data science / analytic / finance): you accumulate data, you trying to find information and pattern into them, this in order to derive model to make prediction. And without surprises the most interesting talks about FinTech came from the people involved in the Bitcoin economy / technology (Blockchain and &lt;a href=&quot;https://www.ascribe.io/&quot;&gt;ascribe Gmbh&lt;/a&gt;). Maybe the banks have some cool stuffs to talk about, but they weren’t really present.&lt;/p&gt;

</description>
        <pubDate>Tue, 24 Nov 2015 17:37:06 +0100</pubDate>
        <link>http://mrbonsoir.github.io//blog/post/2015/11/24/Some-data-about-data-conference.html</link>
        <guid isPermaLink="true">http://mrbonsoir.github.io//blog/post/2015/11/24/Some-data-about-data-conference.html</guid>
        
        
        <category>blog</category>
        
        <category>post</category>
        
      </item>
    
      <item>
        <title>Data conference coming!</title>
        <description>&lt;p&gt;I’m interested in knowing, observing how my field - applied research, technology, imaging, innovation… whatever you call it - is evolving. Working full time on one project is of course a good solution to see what’s going on, but it’s also taking the risk of being stock in daily routines. In that sens it’s always wise to have a look of what your neighbors, competitors are doing, how they try to solve the same problem you are working on.&lt;/p&gt;

&lt;p&gt;From my own experience I know that we - let’s call us/me applied/data scientist - are very fast categorized in sub-fields, as experts and that it is sometimes difficult to extract yourself from the prism of how people are perceiving what you can do. Having said that, to be able to attend events, meet a new crowd, hopefully interesting people, exchange information, re-present yourself, feel how an industry is growing is something vital.&lt;/p&gt;

&lt;p&gt;A few days ago I did spot an event &lt;a href=&quot;http://datanatives.io/&quot;&gt;Data Natives&lt;/a&gt; scheduled in Berlin the coming 19-20 of November 2015. The keywords combination used to introduce the program is almost too perfect: IoT (internet of thing),  FinTech (Financial Technology and not tech from Finland which sounds pretty cool too) and Big Data of course. Needless to say that I’m pretty excited to attend this conference!&lt;/p&gt;

</description>
        <pubDate>Mon, 02 Nov 2015 14:35:06 +0100</pubDate>
        <link>http://mrbonsoir.github.io//blog/post/2015/11/02/Data-conference-coming.html</link>
        <guid isPermaLink="true">http://mrbonsoir.github.io//blog/post/2015/11/02/Data-conference-coming.html</guid>
        
        
        <category>blog</category>
        
        <category>post</category>
        
      </item>
    
      <item>
        <title>Personal insights on color science</title>
        <description>&lt;h3 id=&quot;context&quot;&gt;Context&lt;/h3&gt;
&lt;p&gt;In a &lt;a href=&quot;http://mrbonsoir.github.io/blog/post/2015/10/23/CIC-visiting-Darmstadt.html&quot;&gt;previous post&lt;/a&gt; I talked about the last event in my field I did attend. Now I want to talk about my perception of this domain which is called color science. I’m pretty sure it can be applied to other fields of research as well.&lt;/p&gt;

&lt;p&gt;From the first time I joined this community, from article reader, article contributor to reviewer, committee member and session chair my understanding of what is color science has evolved. One important thing is to stay humble, especially with the new comers. I have been one them, it was impressive. Impressive because you meet the people, authors of research articles that are part of the foundation of you work. You can add a person, a voice to written words, it’s actually pretty cool.&lt;/p&gt;

&lt;p&gt;There aren’t thousand concepts to understand/enter the world of color science. Like in every fields it’s about observation and trying to explain what’s happening. But here it’s all about light - its spectral properties - how we perceive this signal - a single light source to an image in the visible spectrum - and how can we develop robust scientific/engineering &lt;em&gt;stuffs&lt;/em&gt; around it. What I find interesting is to witness what is the new thing coming each year, how a technical improvement can open a door for further applications.&lt;/p&gt;

&lt;h3 id=&quot;color-trends&quot;&gt;Color trends&lt;/h3&gt;
&lt;p&gt;Among the research sub-fields presented at CIC this year I want to come back on four of them.&lt;/p&gt;

&lt;p&gt;There is the recurrent discussion about color metrics, from a purely mathematical/geometrical approach to a more perception-wise approach trying to add an average human appreciation of the difference between two signals. Having a good metric is always helpful to evaluate your algorithm/experiment. Over the years the metrics are evolving, context is important (from display calibration to color textile differences…).&lt;/p&gt;

&lt;p&gt;There is the what I call &lt;em&gt;purely geometrical approach&lt;/em&gt; discussion where having a signal as vector of n values - for n wavelength -  a group of sensors - basic configuration made of three basis like RGB basis - you want to know the value of this signal once projected on the known basis/sensors. From that you can jump into optimization, addressing various problems such as finding the scene illuminant/white point, study metamerism. It seems obvious but it’s not.&lt;/p&gt;

&lt;p&gt;There is printing and 3D printing - there I meant color 3D printing. Just think of how to design a color test-chart for such printing system. HDR display is also coming stronger than ever. What is interesting with these two examples is that they both require to know your workflow, they are the “end” of a process chain: you need to understand the acquisition process to do a good reproduction. Understanding the use of the technology is obviously required.&lt;/p&gt;

&lt;p&gt;On the last paragraph one can add the understanding of gamut mapping and how you “move” into your color space as something very important. For printers you have multi-inks system changing the shape of the color space available. For high resolution TV and HDR screen the color gamut shape may not change a lot - almost - but the variability of screen size, intensity scale, technology available make it difficult - to be understood as something cool and challenging for me - to offer a comfortable experience to the user among the different platforms.&lt;/p&gt;

&lt;p&gt;Now that I’m a bit more in control with the tools/concepts in my field and sub-fields I have the tendency to prefer the projects combining several concepts - like high quality printing and movie post-production - and I always appreciate to hear how the authors are presenting their projects, which story they are telling us.&lt;/p&gt;

</description>
        <pubDate>Sun, 25 Oct 2015 17:37:06 +0100</pubDate>
        <link>http://mrbonsoir.github.io//blog/post/2015/10/25/Personal-insights-on-color-science.html</link>
        <guid isPermaLink="true">http://mrbonsoir.github.io//blog/post/2015/10/25/Personal-insights-on-color-science.html</guid>
        
        
        <category>blog</category>
        
        <category>post</category>
        
      </item>
    
      <item>
        <title>CIC visiting Darmstadt</title>
        <description>&lt;p&gt;What is CIC you may ask yourself? It’s stand for &lt;a href=&quot;http://www.imaging.org/site/IST/Conferences/Color_and_Imaging/IST/Conferences/CIC/CIC_Home.aspx?hkey=d2cf3f19-87b4-4164-8274-c40180e9dfa7&quot;&gt;Color Imaging Conference&lt;/a&gt;, a conference about color and imaging. This year it took place in Darmstadt DE. The last 22 editions always took place in the US, last year it was in Boston MA, two years ago in Albuquerque NM, three years ago in Los Angeles CA, four years ago in San Antonio TX and that’s it for my involvement. Next stop is San Diego CA in November 2016.&lt;/p&gt;

&lt;p&gt;I’m a regular attendee, I joined this community already ten years ago alternating between CGIV, &lt;a href=&quot;http://www.aic-colour.org/&quot;&gt;AIC&lt;/a&gt;, &lt;a href=&quot;http://www.imaging.org/ist/Conferences/ei2016/&quot;&gt;EI&lt;/a&gt; and CIC. Depending of the event you will meet a slightly different crowd or so to say different crowds will meet allowing to go deeper in the various fields represented. But for sure it’s about imaging, color, perception, printing, archiving, image acquisition, color management, camera and display calibration, gamut mapping and more.&lt;/p&gt;

&lt;p&gt;This year almost 200 persons were attending the event in Darmstadt. There is a kind of routine in such event and being part of the committee allows you to see the people interaction with a special look. It’s very special to see the attendees - former colleagues, friends, known members of this community - arriving from everywhere almost - from North America, Europe, Asia, Australia… - and being all jet-lagged. Even if you are traveling in the same time zone you will end up jet-lagged. First of all the schedule is tide and you have to use the “free” time to talk with everybody. Sharing a meal or a beer is usually very appropriate. As a result you barely have time to rest, but the kind of adrenaline you get from meeting the crème de la crème of the color scientists keeps you awake.&lt;/p&gt;

</description>
        <pubDate>Fri, 23 Oct 2015 18:23:06 +0200</pubDate>
        <link>http://mrbonsoir.github.io//blog/post/2015/10/23/CIC-visiting-Darmstadt.html</link>
        <guid isPermaLink="true">http://mrbonsoir.github.io//blog/post/2015/10/23/CIC-visiting-Darmstadt.html</guid>
        
        
        <category>blog</category>
        
        <category>post</category>
        
      </item>
    
      <item>
        <title>About not being an expert as a data scientist and other tech stuffs</title>
        <description>&lt;p&gt;Last evening I did attend a joined Meetup from the Python User Berlin (PUB) and the Zalando Tech Event hosted by Zalando and offering talks on Natural Langage Processing (NLP). Both talks went well and gave two views on the topic: one on the state of the art of the tools for NLP using Python and a second more applied.&lt;/p&gt;

&lt;p&gt;The discussions I add after while enjoying a club mate - la boisson des champions - were equally interesting. First of all I started discussing with a expert of NLP trying to explain why I joined this event and what was my link with NLP. In my very recent job experience at EyeEm I just touched the surface of NLP preparing data for Machine Learning (ML) using nltk together with WordNet, ImageNet. Actually I didn’t do much of text analysis but batching word definition. In that experiment the text analysis will have come after this step and that’s where semantic is jumping into the discussion. Because working with the word dictionary is one side of the problem: you have one word with its definition and often - at least with scientists or engineers - you are in the inverse configuration which is you having words when actually you want to extract a definition, an idea, an information… And I let you google automatic image tagging, deep learning.&lt;/p&gt;

&lt;p&gt;After exchanging ideas and experiences about NLP I did continue seeping the offered mate with one Zalando employee. I was curious - as usual - to understand what it means to be a data scientist here. Because if the definition is very general - a data scientist works with data, we are not experts - it’s interesting to see how many fields we - I’m one of those people - cover in our daily work. Using the same language - e.g. Python - we can go from signal processing, computer vision, image retrieval, NLP, how to deal with Databases - a year ago I wrote on the topic Databases and natural Langage graphs en stock - how to present your results to non expert by doing nice visualization and many more… So if we are not expert we need to be pretty fast I acquiring skills from various fields and/or use the appropriate tools.&lt;/p&gt;
</description>
        <pubDate>Wed, 30 Sep 2015 12:50:06 +0200</pubDate>
        <link>http://mrbonsoir.github.io//blog/post/2015/09/30/About-not-being-an-expert-as-a-data-scientist.html</link>
        <guid isPermaLink="true">http://mrbonsoir.github.io//blog/post/2015/09/30/About-not-being-an-expert-as-a-data-scientist.html</guid>
        
        
        <category>blog</category>
        
        <category>post</category>
        
      </item>
    
      <item>
        <title>Clash of the titans - optimization vs. machine learning</title>
        <description>&lt;h3 id=&quot;in-the-beginning&quot;&gt;In the beginning&lt;/h3&gt;
&lt;p&gt;What is important to know about machine/deep learning problems? First remark to myself is “what are we trying to solve in general?” and then “which method/technique do we choose?” or “which approach is most appropriate to answer a given problem?”.&lt;/p&gt;

&lt;h3 id=&quot;optimization-for-the-people&quot;&gt;Optimization for the people&lt;/h3&gt;
&lt;p&gt;Optimization is widely used to solve complex problems that don’t have an analytic expression. But this doesn’t mean that problems that have an analytic expression couldn’t be solved using optimization.&lt;/p&gt;

&lt;p&gt;Optimization relies on a provided model that “model” with reasonable efficiency a phenomenon (e.g. find the colorant combination of cyan, magenta, yellow and more for a give red, green, blue pixel) or anything you want. You may hear about derivatives, gradient, local minimum, cost function, quadratic form, linearity, non-linearity, iteration and more when you start messing around with optimization.&lt;/p&gt;

&lt;p&gt;And it’s completely possible to use optimization techniques as applied mathematics tools without knowing exactly how they work (e.g. you provide your model and the tools will perform the derivatives for you). In an engineering world you are connecting boxes, each one trying to solve a simple task taking for starting point what the previous is having for output.&lt;/p&gt;

&lt;h3 id=&quot;deep-learning-for-the-people&quot;&gt;Deep learning for the people&lt;/h3&gt;
&lt;p&gt;Deep learning and neural networks let you do something clever with the way to solve your problem. First of all your problem has been defined and described, but optimization techniques did not provide expected results: it’s not fast enough or it’s simply not working. One possibility is that your model isn’t good enough or way to complex.&lt;/p&gt;

&lt;p&gt;The simple idea is to let a system to learn about an ecosystem. To do so we let the algorithms mimicking how our brain is working. The concept of learning is very important here because it is really what we want to achieve. We want that our algorithm learns in a first step by obtaining representative parameters/weights before giving us a result. Then once the learning is finished, for a given entree and with the help of the parameters the algorithm can give us answers. For example is this image an image of a car, an elephant and this with different degrees of confidence.&lt;/p&gt;

&lt;p&gt;A big part of the learning is to prepare the training sample. You can’t just give images to the algorithm. Applied to computer vision, deep learning methods try to extract features from images in a similar way of how we human recognize information in images. This step of features extraction goes by applying multiple filtering on the images and the resulting filtered images, using convolution and tile approaches. At the end you obtain classes of features and it’s very similar to the filters used for face recognition. Only difference is the features that describe a human face are now almost standard and doesn’t need to be computed or extracted again.&lt;/p&gt;

&lt;p&gt;There exist competitions where for a given large database full of images and  keywords, people can submit their algorithms. Pretty interesting results are obtained and as in sport faster solutions are appearing often coming with new tools to handle large databases.&lt;/p&gt;

&lt;h3 id=&quot;breaking-the-machine&quot;&gt;Breaking the machine&lt;/h3&gt;
&lt;p&gt;Hopefully there is always something to improve. Because images can contain more than one object, you could have a bike and an elephant in the same picture. In that case what should reply our algorithm first? There is room for subjectivity here.&lt;/p&gt;

&lt;p&gt;These algorithms have to deal with the constant stream of information we are processing, meaning that we are always learning - in theory of course because the world is full of lazy bastards which keeps the marketing and sales people happy making us predictable and therefore easy targets but I digress - and we have to find a way to give this ability to our algorithms or there is the risk for them to over-learn. I really like the metaphor of trying to make an algorithm able to forget part of what he is deep learning to be able to adjust its judgement.&lt;/p&gt;

&lt;p&gt;The interesting problems are those that overcome the first limitations encountered. You could try to distinguish what are the elements in a picture (e.g. there is an elephant and a bike) or “simply” give to the image a score. If you take an artist, he will have the tendency not only to make the same picture but to add to its images something that defines the way he perceive the world around him, something pretty unique. A similarity factor or score can be very helpful when you are browsing a large image databases or “just” the internet.&lt;/p&gt;
</description>
        <pubDate>Wed, 15 Apr 2015 13:17:06 +0200</pubDate>
        <link>http://mrbonsoir.github.io//blog/post/2015/04/15/Clash-of-the-titans.html</link>
        <guid isPermaLink="true">http://mrbonsoir.github.io//blog/post/2015/04/15/Clash-of-the-titans.html</guid>
        
        
        <category>blog</category>
        
        <category>post</category>
        
      </item>
    
      <item>
        <title>Deep learning (ou deep learning in French)</title>
        <description>&lt;h3 id=&quot;what-was-your-question-already&quot;&gt;What was your question already?&lt;/h3&gt;
&lt;p&gt;How to explain deep learning to your friends, family members, neighbors, random stranger, dog? A very good question indeed. Rather than going deeply into neural networks and other festivities let’s start with describing the problem we want to solve or at least let’s give an example of what we are trying to do here.&lt;/p&gt;

&lt;p&gt;Over the years I had to come with strategies if I wanted to explain what I do for living. Telling combinations of words such as “color science”, “computer vision”, “image processing”, “digital photography” is usually not enough or saying “I do work with images” neither. I always found interesting to answer the question “why to you want to do that?” or “which problem do you want to solve?” instead of saying I’m this or that. So to explain what I can do I try to give an idea of the tasks I have to solve.&lt;/p&gt;

&lt;h3 id=&quot;what-is-the-problem-you-are-trying-to-solve-already&quot;&gt;What is the problem you are trying to solve already?&lt;/h3&gt;
&lt;p&gt;In some way asking these questions is already machine learning/deep learning-ish approach of solving a problem. In theory if someone asks you to solve a problem he knows the kind of results he want to obtain for a given input or starting point. What he doesn’t know is what is happening between these two stages. Applied mathematics and optimization are a reasonable standard solution: you develop of model that recreate more-less accurately what is happening between these two stages, then for a new entree point your model will predict what an output will be.&lt;/p&gt;

&lt;p&gt;I’m sure &lt;em&gt;big data&lt;/em&gt; is an expression you have heard in the past years or months. It has of course different meaning depending who is giving a definition. But, coming back to images and the incredible amount of images we are producing daily there is a need to develop solutions/tools to be able to interact with these images. You have in your hand an extremely large image database and using keyword as a search query isn’t enough anymore. Here is the problem: how to navigate, how to browse into large image database in a more natural way? There is a bit of database here but that is not the main point of my article, check my past post on graph and database if you are interested.&lt;/p&gt;

&lt;h3 id=&quot;face-recognition-to-recognition-of-everything&quot;&gt;Face recognition to recognition of everything&lt;/h3&gt;
&lt;p&gt;Working with images is fascinating, you see one image and automatically you extract some of its  information. Of course there is a long learning curve, when you see a tree, a car, a known object in a picture you don’t even realize it, you know, you have learned over the years you spent on earth to recognize, categorize, organize the continuous stream of visual information that come to your eyes and is later processed in your brain.&lt;/p&gt;

&lt;p&gt;If you think of face recognition, the mathematical tools are now pretty standard. We can with high probability find out faces in images, classification comes after the recognition. And if you train your model you will be able to recognize semi automatically in a database faces of different persons as the tools/filters can be tuned for a given target. It can be scary of course if the threshold that decide for a true recognition/classification isn’t verified by a real human and that action leads to a rocket launch. Actually any automatic action issued from an algorithm decision having impact on a human being is pretty bad (hello mass surveillance and hello Terminator). You want help from robots not to help robots or it’s too late anyway.&lt;/p&gt;

&lt;iframe src=&quot;https://player.vimeo.com/video/12774628&quot; width=&quot;500&quot; height=&quot;477&quot; frameborder=&quot;0&quot; webkitallowfullscreen=&quot;&quot; mozallowfullscreen=&quot;&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;p&gt;&lt;a href=&quot;https://vimeo.com/12774628&quot;&gt;OpenCV Face Detection: Visualized&lt;/a&gt; from &lt;a href=&quot;https://vimeo.com/adamhrv&quot;&gt;Adam Harvey&lt;/a&gt; on &lt;a href=&quot;https://vimeo.com&quot;&gt;Vimeo&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;An idea behind deep learning is to be able to learn what are into images - in a similar way as we human do - to extract features and to perform tasks on other images based on a trained neural network. I’m making shortcuts but that’s the idea. To understand and to later mimic how information is circulating into the brain has been a dream of many researchers. Neural networks go into that direction. If a few years ago the algorithms were limited because of computer power the global picture is different now.&lt;/p&gt;

&lt;p&gt;What is also interesting is that new strategies had to be developed to overcome the overload of data. In a way the systems were “over learning” and people talked about over-fitting the data. And it makes sens. If I’m not too mistaken our brain is not indefinitely expandable, meaning we are sorting information continuously. One big part of these tools is to perform drop-out which can be explained as “now that our system can learn we have to teach him to forget part of what he knows in real time”.&lt;/p&gt;

&lt;h3 id=&quot;cross-disciplines&quot;&gt;Cross disciplines&lt;/h3&gt;
&lt;p&gt;A chance I see - for me - is the need in some industries for expert being not only expert in one field. Specially for this kind of large scale problems involving images, computer vision, real time and fancy applied research projects. To know only about machine learning or statistic is not enough, to know both about computer and machine learning tools is better.&lt;/p&gt;

</description>
        <pubDate>Thu, 09 Apr 2015 13:28:06 +0200</pubDate>
        <link>http://mrbonsoir.github.io//blog/post/2015/04/09/Deep-learning.html</link>
        <guid isPermaLink="true">http://mrbonsoir.github.io//blog/post/2015/04/09/Deep-learning.html</guid>
        
        
        <category>blog</category>
        
        <category>post</category>
        
      </item>
    
  </channel>
</rss>
